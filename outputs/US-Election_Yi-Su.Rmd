---
title: "Forecasting US Election: Trump or Biden"
subtitle: "TBD"
author: "Yi Su"
thanks: "Code and data are available at: https://github.com/YiSu2000/US_Election_Yi_Su"
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: |
  | First sentence. Second sentence. Third sentence. Fourth sentence.
  | **keywords:** Forecasting; US 2020 Election; Donald J. Trump; Joe Biden; Multilevel Regression with Post-stratification
output:
  bookdown::pdf_document2:
bibliography: references.bib
toc: FALSE
header-includes:
 \usepackage{float}
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE, warning=FALSE}
library(knitr)
library(broom)
library(skimr)
library(naniar)
library(tidyverse)
library(haven)
library(tidybayes)
library(ggplot2)
library(brms)
library(statebins)
library(kableExtra)
library(data.table)
#packages used

sample_data <- read_csv("/Users/jordans2000/Desktop/STA304/PS4-Election/US_election_Yi_Su/inputs/data/reduced_data.csv")
# the sample data from reducing the full survey dataset from Democracy Fund and UCLA Nationscape.
ps_data <- read_csv("/Users/jordans2000/Desktop/STA304/PS4-Election/US_election_Yi_Su/inputs/data/reduced_data_post_strat_1.csv")
# post-stratify dataset reduced from the ACS dataset.
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r survey data, include=FALSE, echo = FALSE, message = FALSE, warning=FALSE}
#we are only interested in whether they vote for Trump or Biden, so we would like to treat all observations saying "I will not vote" and "I am not sure" as missing values. This reduces the sample size to 4753 observations of individuals from 6480.
sample_data <- sample_data%>%
  replace_with_na(replace = list(
    vote_2020 = c("I am not sure/don't know", "I would not vote", "Someone else")))%>%
  na.omit()
ps_data <- ps_data%>%na.omit()
#also omit missing values in ps_data
sample_data$Who <- ifelse(sample_data$vote_2020 == "Donald Trump", 1, 0)
#Construct binary dummy variable, 1 for vote for Trump and 0 for vote for Biden
#We are interested in regress voting for Trump or Biden on age groups, household income level, race, and states in the U.S..
# For the codes below, I am aware that these might be the more "ugly" or wordy approach, but this is simply the easiest to interpret and modified if needed later.

#first create household income levels 
sample_data <- sample_data%>%
  mutate(incgrp = recode(household_income,
                         'Less than $14,999' = "Less than $35,000",
                         '$15,000 to $19,999'= "Less than $35,000",
                         '$20,000 to $24,999'= "Less than $35,000",
                         '$25,000 to $29,999'= "Less than $35,000",
                         '$30,000 to $34,999'= "Less than $35,000",
                         '$35,000 to $39,999'= "$35,000 to $69,999",
                         '$40,000 to $44,999'= "$35,000 to $69,999",
                         '$45,000 to $49,999'= "$35,000 to $69,999",
                         '$50,000 to $54,999'= "$35,000 to $69,999",
                         '$55,000 to $59,999'= "$35,000 to $69,999",
                         '$60,000 to $64,999'= "$35,000 to $69,999",
                         '$65,000 to $69,999'= "$35,000 to $69,999",
                         '$70,000 to $74,999'= "$70,000 to $99,999",
                         '$75,000 to $79,999'= "$70,000 to $99,999",
                         '$80,000 to $84,999'= "$70,000 to $99,999",
                         '$85,000 to $89,999'= "$70,000 to $99,999",
                         '$90,000 to $94,999'= "$70,000 to $99,999",
                         '$95,000 to $99,999'= "$70,000 to $99,999",
                         '$100,000 to $124,999'= "$100,000 to $174,999",
                         '$125,000 to $149,999'= "$100,000 to $174,999",
                         '$150,000 to $174,999'= "$100,000 to $174,999",
                         '$175,000 to $199,999'= "$175,000 to $249,999",
                         '$200,000 to $249,999'= "$175,000 to $249,999",
                         '$250,000 and above'= "More than $250,000"))

#Also modify race to get inline with post-strat. dataset
sample_data <- sample_data%>%
  mutate(race = recode(race_ethnicity,
                       'Asian (Chinese)' = "Chinese or Japanese",
                       'Asian (Japanese)'= "Chinese or Japanese",
                       'Asian (Asian Indian)'= "Other Asian or Pacific Islander",
                       'Asian (Filipino)'= "Other Asian or Pacific Islander",
                       'Asian (Korean)'= "Other Asian or Pacific Islander",
                       'Asian (Vietnamese)'= "Other Asian or Pacific Islander",
                       'Asian (Other)'= "Other Asian or Pacific Islander",
                       'Pacific Islander (Native Hawaiian)'= "Other Asian or Pacific Islander",
                       'Pacific Islander (Guamanian)'= "Other Asian or Pacific Islander",
                       'Pacific Islander (Samoan)'= "Other Asian or Pacific Islander",
                       'Pacific Islander (Other)'= "Other Asian or Pacific Islander",
                       'Black, or African American' = "Black/African American/Negro",
                       'White' = 'White',
                       'American Indian or Alaska Native' = "American Indian or Alaska Native",
                       'Some other race' = "Some other race"))
#and we'd also like to group ages into 8 decades
sample_data <- sample_data %>% 
  mutate(agegrp = case_when(age < 20 ~ "Under 20",
                            age >= 20  & age < 30 ~ "Between 20 to 30",
                            age >= 30  & age < 40 ~ "Between 30 to 40",
                            age >= 40  & age < 50 ~ "Between 40 to 50",
                            age >= 50  & age < 60 ~ "Between 50 to 60",
                            age >= 60  & age < 70 ~ "Between 60 to 70",
                            age >= 70  & age < 80 ~ "Between 70 to 80",
                            age >= 80  ~ "Above 80"))
#These code on state names are enlighten by Alexander Rohan's Piazza answer   
state_names <- tibble(statefull = state.name, state = state.abb)   
sample_data <-sample_data%>%left_join(state_names)         
```

```{r poststrat_data, include=FALSE, echo = FALSE, message = FALSE, warning=FALSE}
ps_data <- ps_data%>%
  replace_with_na(replace = list(hhincome = 9999999))%>%
  na.omit()
ps_data <- ps_data%>%
  filter(age >= 18 )
#To capitalize the initial character of each state
simpleCap <- function(x) {
  s <- strsplit(x, " ")[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2),
      sep="", collapse=" ")
}
ps_data$statefull <- sapply(ps_data$stateicp, simpleCap)
#Recode D.C. to Washington since the sample dataset doesn't recognize D.C.
ps_data <- ps_data%>%
  mutate(statefull = recode(statefull,
   'District Of Columbia' = "Washington"
  ))
#also create income levels since the post-strat. household income are numeric
ps_data <- ps_data%>%
  mutate(incgrp = case_when(
    hhincome < 35000 ~ "Less than $35,000",
    hhincome >= 35000 & hhincome < 70000 ~ "$35,000 to $69,999",
    hhincome >= 70000 & hhincome < 100000 ~ "$70,000 to $99,999",
    hhincome >= 100000 & hhincome < 175000 ~ "$100,000 to $174,999",
    hhincome >= 175000 & hhincome < 250000 ~ "$175,000 to $249,999",
    hhincome >= 250000 ~ "More than $250,000"
  ))
#similarly, create age groups accordingly to the survey sample dataset
#this only has 7 groups for reasons to be discussed in section 2.1
ps_data <- ps_data %>% 
  mutate(agegrp = case_when(age < 20 ~ "Under 20",
                            age >= 20  & age < 30 ~ "Between 20 to 30",
                            age >= 30  & age < 40 ~ "Between 30 to 40",
                            age >= 40  & age < 50 ~ "Between 40 to 50",
                            age >= 50  & age < 60 ~ "Between 50 to 60",
                            age >= 60  & age < 70 ~ "Between 60 to 70",
                            age >= 70  ~ "Above 70"))
#create race groups
ps_data <- ps_data%>%
  mutate(race = recode(race,
                       'chinese' = "Chinese or Japanese",
                       'japanese'= "Chinese or Japanese",
                       'other asian or pacific islander' = "Other Asian or Pacific Islander",
                       
                       'black/african american/negro' = "Black/African American/Negro",
                       'white' = 'White',
                       'american indian or alaska native' = "American Indian or Alaska Native",
                       'other race, nec'= "Some other race",
                       'two major races'= "Some other race",
                       'three or more major races'= "Some other race"))
#then we add proportions to the three group level variables we choose, that are race, age, and states
counts <- ps_data%>%count(agegrp, race, statefull,incgrp)
#post-stratify each group, counting the proportions stratified by each group
#counting proportion based on age group
agegrp_p <- counts %>% 
  group_by(agegrp) %>% 
  mutate(prop = n/sum(n)) %>% 
  ungroup()
#based on states
state_p <- counts %>% 
  group_by(statefull) %>% 
  mutate(prop = n/sum(n)) %>% 
  ungroup()
#based on race
race_p <- counts %>%
  group_by(race) %>% 
  mutate(prop = n/sum(n)) %>% 
  ungroup()
#based on income group
incgrp_p <- counts %>% 
  group_by(incgrp) %>% 
  mutate(prop = n/sum(n)) %>% 
  ungroup()
```

# Introduction

President election has been one of the most important political events in the United States of America which happens every 4 years. Most American citizens will be involved in this event and make their decision on which direction the county will go at least in the next 4 years. Meanwhile, the rest of the world will also keep their eye on the election because of the global political position of the US. The election is an invisible war between the parties of the US, among those parties, the Republican and the Democratic are the two oldest and dominant parties.

The Republican party and the Democratic party are the two dominant parties holding a large number of positions in congress. Throughout the history of the US, the competition between the president nominates of these two parties has never stopped. In 2016, Donald J. Trump won the election as the nominate of the Republican party and defeating his opponent from the Democratic party, Hilary Clinton. In 2020, Trump is the Republican nominate again and this time, his main opponent from the Democratic party is Joe Biden, a former vice president of the U.S. during 2009-2017. In general, these two nominates are more likely to win the election than nominates from other parties like the Green party. 

In this report, we are interested in forecasting the 2020 U.S. election. Specifically, who is more likely to win among Trump and Biden? To do this, the support of a voter intent survey is essential, and we used the UCLA Nationscape survey dataset [@citesurvey] requested from the URL in reference. The discussion of this survey dataset will be included in *Section 2.1*. Among the many statistical techniques of forecasting election, we used multilevel regression and post-stratification   (MRP) to produce estimates of votes. The MRP involves partitioning the data into small cells based on demographic characteristics of our choice, then estimate voter intent (Trump or Biden) in the cell level using a multilevel regression model. To make the forecasts, we used a census-like dataset, the American Community Survey Dataset [@citepsd]. This is the dataset where partitioning into cells happens and we make predictions from here. The discussion on this dataset is included in *Section 2.2*. 

The discussion on the specific procedure of this report and the multilevel regression model is included in *Section 3*, which we regressed the voting intent based on age, household income, race and states in the US. In *Section 4*, we present and discuss the resultant model and present estimates of the voting intent between Trump and Biden. Meanwhile, *Section 5* includes discussions of our forecast results as well as some weaknesses and future improvements on our procedure. This report was produced using `R` [@citeR], as well as some packages which will be mentioned in each section for usage.

The forecast results are divided into two main part, the first part is an overall support rate among the ACS dataset in favor of Trump. The second part is to calculate the mean support rate of Trump in each state and then calculate the number of electoral votes from each state that we forecast Trump to get.

# Data

## Democracy Fund + UCLA Nationscape Data 

The Nationscape [@citesurvey] is a survey conducted from July 2019 to December 2020, collecting demographics of the respondent as well as their voting intent during the 2020 election. The survey samples are provided by Lucid, an online exchange platform focusing on market research. Specifically, the samples were drawn from the online platform based on a set of demographic quotas like age, region, and gender.

The Nationscape aimed at conducting 500,000 interviews in total and roughly 6,250 interviews per week. The survey took the form of an online survey using a survey software controlled by the Nationscape team, however, the respondents were sent to the software directly by the Lucid platform. Since only the respondent will only be directed to the survey software if they match on the Lucid platform, the non-response rate should be reasonably low. Although the quality of the responses is expected to be high from the Lucid platform, the representativeness of the population of interest still needs to be assessed. This is solved by comparing the Nationscape's results to the results of the Pew Research Center’s evaluations of online non-probability samples in 2018. The Pew Research Center’s 2018 report assessed how various choices impact the quality of the online survey.

After requesting the data on the June-25-2020 phase of the Nationscape survey dataset. We need to modify the original observation levels based on our needs.  In this report, we are interested in modeling the voting intent between Trump and Biden by age, household income, race, and states of living. Only 4 variables were chosen because of some hardware limitations which will be discussed in *Section 3* in detail. The original survey data has race and income levels too narrow and thus might cause trouble in our regression.

First, we created a binary variable with 1 meaning vote for Trump and 0 meaning vote for Biden. This will be the response variable of our regression model. We deleted the observations that will vote for nominates other than Trump and Biden, and this caused a reduction in sample size for our model. Further discussion of the potential hazard of this reduction is included in *Section 5.4*.

Second, to make cells wider for a stable sample mean in the MRP process, we redefined some variables related to age, race, and household income. We reduced the household income levels from 24 to 6 wider income groups. For the race, the categories for races are reduced from 15 to 5 wider race groups. Similarly, age was redefined from a discrete variable to a categorical variable with 8 levels, each level representing a decade's group of age. For specific of the group names of each variables, refer to Table \@ref(tab:tab2) which is frequency table that includes all variable group names except for the states. Since the state is an important factor in the election and we need individual state forecast, there will be no modification on it. 

The decision on how many level or group there is for each explanatory variable is based on both the Nationscape dataset and the ACS dataset. For example, since household income is a categorical variable in the Nationscape dataset, we need to categorize the household income numeric variables in the ACS dataset so the two dataset has the exactly same variable categories for forecasting. Another good example is with race, since the ACS dataset groups Chinese and Japanese into one category, we need to group the Chinese and Japanese samples in the Nationscape dataset into one category as well.

Some example observations in the dataset are shown in Table \@ref(tab:tab1)). These modifications of the dataset were done with `R` [@citeR], through R packages `tidyverse` [@citetidyverse], `naniar` [@citenaniar], `haven` [@citehaven] and `broom` [@citebroom]. And Table \@ref(tab:tab1)) was created with `knitr` package [@citeknitr] and `kableExtra` package [@citekable].


```{r tab1, echo = FALSE, message = FALSE}
options(knitr.kable.NA = '')
#turn NA's in the table to spaces
#kable showing example rows of the modified dataset
kable(sample_data[1:6,c(5, 18, 19, 20, 21, 22)], 
      col.names = c("Expected Vote in 2020", "", "Income level", "Race", "Age group", "State"), 
      caption = "the first 6 rows of the dataset", 
      align = "c")%>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))
  #kable_styling helps the layout of the kable
```

```{r echo= FALSE, message = FALSE, warning = FALSE, error = FALSE}
z1 <- data.frame(table(sample_data$agegrp))
z2 <- data.frame(table(sample_data$incgrp))
z3 <- data.frame(table(sample_data$race))
z4 <- data.frame(table(sample_data$statefull))
#create table of counts of each category level/group
h1 <- merge(data.frame(z1[c(8,2,3,4,5,6,7,1),], row.names=NULL), 
            data.frame(z2[c(5,3,4,2,1,6),], row.names=NULL),
  by = 0, all = TRUE)
h2 <- merge(data.frame(h1, row.names=NULL), 
            data.frame(z3, row.names=NULL),
  by = 0, all = TRUE)
h3 <- merge(data.frame(h2, row.names=NULL), 
            data.frame(head(z4, 8), row.names=NULL),
  by = 0, all = TRUE)
#merging all the tables, only first 8 rows for the states
```

Next, we check the frequency of each group in our modified dataset to ensure variation in explanatory variables for the accuracy of the model. Table \@ref(tab:tab2) displays the frequency of each group within each variable instead of plotting the distribution of each categorical variables. These frequencies tells us the distribution of each categorical variables and provides a more compact view to the problems within each variable. It is also a good display of all variable sub-group names. However, since there are 50 states, we only display the first 6 states in alphabetic order. 

Although only 6 states are shown, the problem is clear. Some states like Alaska and Arkansas have a significantly fewer number of respondents than large states like California. The same situation happened in all other variables as well, White people have way more number of observations than Chinese or Japanese and Alaskan natives, the number of respondents above 80 is way less than others, and the number of respondents with higher household income decreases as income level increase. 

Unfortunately, we can only redefine the age groups since the other variables groups need to line up exactly the same between the two datasets. And some variables are already at the base-line level and could not be further modified.  We can choose to specify states into different regions, but that would obey one of our goal of forecasting election base on electoral votes which is highly dependent on winning in each state.

After we joined the age group of above 80 into the age group between 70 to 80. The remaining differences in other variables would be small enough to be compensated by our Bayesian multilevel modeling approach. This approach pools the effect of the minor groups with other more major group cells, which is very beneficial for the states with less than a hundred respondent observations since we cannot redefine state groups.

```{r tab2, echo = FALSE, message = FALSE}
options(knitr.kable.NA = '')
kable(h3[1:8,4:11], 
      col.names = c("Age Group", "Count", "Income level", "Count", "Race","Count", "State", "Count"),
      align = "c", 
      caption = "Frequency of each group")%>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))
#similar to the previous table chunk
```

```{r echo = FALSE, message = FALSE}
sample_data <- sample_data %>% 
  mutate(agegrp = case_when(age < 20 ~ "Under 20",
                            age >= 20  & age < 30 ~ "Between 20 to 30",
                            age >= 30  & age < 40 ~ "Between 30 to 40",
                            age >= 40  & age < 50 ~ "Between 40 to 50",
                            age >= 50  & age < 60 ~ "Between 50 to 60",
                            age >= 60  & age < 70 ~ "Between 60 to 70",
                            age >= 70  ~ "Above 70"))
#now we only have 7 levels of age group since we combined the last two groups
```

Further concerns with this pick of variables are any underlying strong multicollinearity between age and income. We roughly check this by Figure \@ref(fig:fig1), which was created with the `ggplot2` package [@citeggplot] and `data.table` package [@citedata]. We do see an increasing income level trend with aging in general. However, the degree of multicollinearity is not sufficient to violate the assumption of no perfect multicollinearity between explanatory variables since the general proportion of incomes remains steady except for the age group between 20 to 30.

```{r fig1, fig.cap = "Percentage of income levels for age groups", echo = FALSE, message = FALSE, out.width="80%"}
nset <- setDT(sample_data)[,list(count = .N), by = .(agegrp,incgrp)][,list(incgrp = incgrp, count = count, percent_fmt = paste0(formatC(count*100/sum(count), digits = 3), "%"),percent_num = count/sum(count)), by = agegrp]
#create percentage dataset in a similar fashion of the poststratified dataset, but this time only with two groups stratified.
nset%>%
  ggplot(aes(x = factor(agegrp,
                        levels = c("Under 20",
                                   "Between 20 to 30",
                                   "Between 30 to 40",
                                   "Between 40 to 50",
                                   "Between 50 to 60",
                                   "Between 60 to 70",
                                   "Above 70")), 
             #factor to display the levels in order
             y = percent_num,
             fill = factor(incgrp,
                           levels = c("Less than $35,000",
                                      "$35,000 to $69,999",
                                      "$70,000 to $99,999",
                                      "$100,000 to $174,999",
                                      "$175,000 to $249,999",
                                      "More than $250,000"))))+
  geom_bar(position = position_fill(reverse=FALSE), 
           stat = "identity")+
  #barplot fill with percentage
  coord_flip()+
  geom_text(aes(label = percent_fmt),
            position = position_fill(vjust = 0.5), 
            size = 4,
            check_overlap = TRUE)+
  # turn on check overlap to avoid any overlap
  scale_fill_discrete(name = "Income level")+
  xlab(label = "Age Group")+
  ylab(label = "Percentage")
```

## American Community Surveys Data

The American Community Survey (ACS) [@citepsd] are monthly surveys on rolling households that was designed as a substitute of a census. The target population of the ACS is all American households since it is designed to replace a census, and at full implementation, the sample would include about 3 million households across the US. The samples were extracted from the Census Bureau's larger internal data files, and thus shares the same sampling errors. For each sampled household, the ACS records the response from all member of the family. Privacy was protected by restricting geographic variables to state level and some of the individual variables are Top coded.

The ACS sample design took form of systematically sampling one household to represent each U.S. county each month. And that monthly sample will receive the ACS survey though mail at the beginning of the month. Non-respondents of the mail were contacted via telephone for a computer assisted telephone interview one month later. If the household is still not responding, one third of the non-respondents to the mail or telephone survey are contacted in person for a computer assisted personal interview one month following the last attempt. Since the ACS is sampling on a county level monthly, this makes it close to a census dataset. However, the initial attempt of survey through mailing may increase the initial non-response rate and decrease quality of response. Mailing back takes more effort as the respondent and mailing also have the uncertainty of lost mails during delivery. In the most extreme cases, the survey mail may be miss-classified as a junk mail and thus discarded. However, the latter attempts would likely to solve this issue, but requiring more effort.

The ACS dataset is as good as a census dataset and thus appropriate to our goal of forecasting for the whole nation's vote. For our purpose, we need to filter out all sample observations with age under 18 so the remaining all reaches the age to vote, and then assume all observation in the remaining dataset will vote between Trump and Biden. The next step is to create variable groups that are identical to ones we created in the Nationscape survey dataset. Ensuring the variables in both dataset lines up is how we could implement our model trained from the Nationscape dataset to the ACS dataset and create the estimated forecasts. 

First we notice that the recorded household income in the ACS dataset are numeric variables and we need to first remove the observations with value *9999999* which is a special way of recording missing values. Then we could group the observations to 6 groups by the same logic statements on the numeric income values used in the modified Nationscape dataset. The procedure of grouping the race and age variables is identical to how we grouped the Nationscape dataset. The categorical groups of race was reduced from 9 groups to 6 groups by merging some smaller groups together. The age variable is also grouped to 7 decade groups identical to the modified Nationscape dataset. The above procedure gives a modified ACS dataset that have the identical categorical variables as the modified Nationscape dataset but with a lot more observation. Thus, Table \@ref(tab:tab1) can also be used as example observations of the modified ACS dataset. 

Table \@ref(tab:tab3) shows the frequency of each group within the explanatory variables in a similar fashion of Table \@ref(tab:tab2) in the previous section. The reason why use a frequency table instead of plotting each variable is the same as for Table \@ref(tab:tab2).

```{r echo= FALSE, message = FALSE, warning = FALSE, error = FALSE}
a1 <- data.frame(table(ps_data$agegrp))
a2 <- data.frame(table(ps_data$incgrp))
a3 <- data.frame(table(ps_data$race))
a4 <- data.frame(table(ps_data$statefull))
#create table of counts of each category level/group
t1 <- merge(data.frame(a1[c(7,2,3,4,5,6,1),], row.names=NULL), 
            data.frame(a2[c(5,3,4,2,1,6),], row.names=NULL),
  by = 0, all = TRUE)
t2 <- merge(data.frame(t1, row.names=NULL), 
            data.frame(a3, row.names=NULL),
  by = 0, all = TRUE)
t3 <- merge(data.frame(t2, row.names=NULL), 
            data.frame(head(a4, 8), row.names=NULL),
  by = 0, all = TRUE)
#merging all the tables, only first 8 rows for the states
```

```{r tab3, echo = FALSE, message = FALSE}
options(knitr.kable.NA = '')
kable(t3[1:8,4:11], 
      col.names = c("Age Group", "Count", "Income level", "Count", "Race","Count", "State", "Count"),
      align = "c", 
      caption = "Frequency of each group")%>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))
```

The distribution of the groups are similar to the Nationscape dataset, and the unequal distribution of frequency in each group still exist as expected. There still exist states that have significantly lower frequency comparing to larger states, the higher house income level groups still have lower frequency among the dataset. Chinese or Japanese and Alaska Native or American Indians still have a significantly lower share of the distribution comparing to the number of White people.

However, this similar pattern is a good sign which indicate that our regression model trained on the Nationscape dataset would be appropriate to use on the ACS dataset. This similarity also indicates the reliability of both dataset in term of response quality. Since the distribution of variables though the two survey match, we have more reason to believe that the two survey captured the true population distributions in general.

The next step is the post-stratification part of the MRP approach. The first step is to sum up all the observations in the dataset sharing the same age group, income group, race and state. Then we can create new datasets with calculated proportion of such combination of variables with respect to the specific variable we are interested in. Table \@ref(tab:tab4) shows one of such sub-dataset for proportion with respect to age groups. For example, for the first row, the proportion variable means that the proportion of American Indian or Alaska Native living in Alabama with income between \$100,000 to \$174,999 at age above 70 is only 0.00025% of all people with age above 70. This could also be done by calculating the proportion of that cell with respect to the whole ACS dataset samples, that will make the proportion be $n/sum(n)$. The former group wise proportion is useful when making predictions within each group, while the latter is useful when making a final prediction of proportion of Trump vote among all the samples in the ACS dataset.

```{r tab4, echo = FALSE, message = FALSE}
kable(agegrp_p[1:6,],
      col.names = c("Age Group", "Race", "State", "Income Group", "Count", "Proportion"),
      align = "c", 
      caption = "Proportion with respect to Age")%>%
  kable_styling(latex_options = c("HOLD_position","scale_down"))

```

# Model

```{r model, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}
#due to the ability of my laptop, if I try adding more layers to other variables my laptop would fail to do so.....So I decided to only add layers on age group, states, and race.
#Bayesian model with multi-leveling on age, income, race and state using the equation in this section
model8 <- brm(Who ~ (1|agegrp) + (1|incgrp) + (1|statefull) + (1|race), 
            data = sample_data, 
            family = bernoulli(),
            control = list(adapt_delta = 0.98),
            #regression on vote for trump (binary)
            seed = 6431,
            #seed not perfect in brm, but better than nothing
            silent = TRUE,
            refresh = 0,
            #the above option turns off all message completely wen knitting
            file = "/Users/jordans2000/Desktop/STA304/PS4-Election/US_election_Yi_Su/outputs/model/model8")
#file the model so it doesn't run every time
summary(model8)
```
A multilevel logistic regression model was fitted on the modified ACS dataset using a Bayesian approach. The regression was done in `R` [@citeR] using the `brms` package [@citebrms] and `tidybayes` package [@citetidybayes]. We use a logistic regression since our response variable is a binary variable, with value 1 representing vote for Trump and 0 representing vote for Biden. Our explanatory variables are age groups, race, household income groups, and states. 

The choice of age as a explanatory variable is based on modern history of the US. The US is one of the few countries that have experience of wars in the recent decades although they are not domestic wars. This experience of war is likely to influence individual's opinion about Trump, since the Nationscape dataset do not include veteran status, we use age groups instead and hoping that higher age groups may have higher proportions of veterans whether in the WWII or recent ones. The choice of household income and race are also based on some expected difference of opinions about Trump when they have different income level or race. Because of some recent event between races, we do expect a significantly less support rate for Trump among American Africans comparing to other races.

However, these choice of explanatory variables are also due to limitation of hardware. Unfortunately, the regression needed to be run on local personal devices with poor CPU performance and limited amount of memory. Four categorical variables with an intercept for each is the maximum performance that the device could get after numerous attempts of more complex regressions. We made attempts including gender, labor force status, education and citizenship status. Most models are either successfully built but exceeding our device memory when making forecasts or it breaks when modeling on our device. The limitations result by this constraint will also be discussed in *Section 5.4*.

\begin{equation}
Pr(y_{i}=1) = logit^{-1}(\alpha^{age\,group}_{a[i]}+ \alpha^{income\,group}_{b[i]}+ \alpha^{race}_{c[i]}+ \alpha^{state}_{d[i]}) (\#eq:bayes)
\end{equation}

Equation \@ref(eq:bayes) displays the logistic regression model. Each $\alpha$ represent the age group, race, household income group and state respectively. And the subscripts $(a,b,c,d)[i]$ represent the specific sub-group that the $i^{th}$ individual belong to. For example, if we have $\alpha^{age\,group}_{Below\,20[j]}$, this means the individual j belong to the age group of below 20. However, the $(a,b,c,d)$ would be replaced with number 1 to K, for K be the number of total number of categorical groups in that variable.

Each of the $\alpha$ are modeled as normally distributed with mean 0 and a variance which we set prior for. For example, the $\alpha$ for the age group variable would be modeled by:
$$\alpha^{age\,group}_{a[i]}\sim {Normal}(0, \sigma_{age\,group})\,for\,a= 1,2,...,7$$

Since we are using a Bayesian approach model, we could set prior distribution on the variance of these $\alpha$s. In this report, we use the default prior distribution of the `brms` package function `brm` which are student t distributions since have no better prior information to provide. 

The `brms` package essentially bring the merit of `Stan` using simple `R` syntax and through similar algorithms like Markov chain Monte Carlo (MCMC). MCMC chains allow the model to develop the parameter gradually, 4 chains and 1000 iteration (not including warm up) were used to fit the model with the help of `brm` function in `brms`.  

To ensure convergence of the regression, we modified the The former regression models with lower flexibility had 1-20 divergent transitions in the Bayesian set up, we increased the flexibility of the model by much to achieve total convergence among all transitions during the MCMC method. However, this does not solve the root issue that our model is potentially not a very good fit to the data. Ideally, we could achieve a better fit by adding more control variables in our regression. But as discussed above, the limitation that we had to run the regression locally on our personal devices restricted the degree of layers and explanatory variables we could add to the regression due to the poor performance of hardware.

```{r trace, fig.cap = "Convergence of Model Parameters",echo = FALSE, message=FALSE, warning=FALSE,out.width="80%"}
mcmc_plot(model8, type = "trace")

```

Figure \@ref(fig:trace) shows the trace plot of parameters for the final regression model we use. We successfully eliminated all divergent transitions during the chain, but the traces are still not quite ideal. 

# Results

## Resultant Regression Model

```{r naive, echo = FALSE, message = FALSE, warning = FALSE}
counts <- counts%>%
  mutate(prop = n/sum(n))
#create proportion that is to the whole population instead of group level
fc <- model8 %>%
  predict(newdata = counts)
#make predictions
predictions <- merge(data.frame(counts, row.names=NULL), 
            data.frame(fc, row.names=NULL),
            by = 0, all = TRUE)
#merge the two data frames
TrumpForecast <- sum((predictions$prop)*(predictions$Estimate))
#overall forecast of the proportion of population that will vote for Trump
```

##Forecast of overall support rate of Trump

```{r predictions, echo = FALSE, message = FALSE}
#prediction for each age group
pre_age <- model8 %>%
  add_predicted_draws(newdata=agegrp_p,
                      allow_new_levels=TRUE) %>%
  #create predictions
  rename(vote_predict = .prediction) %>% 
  mutate(vote_predict_p = vote_predict*prop) %>% 
  #weight prediction according to proportion of the var in post-strat dataset
  group_by(agegrp, .draw) %>% 
  summarise(vote_predict = sum(vote_predict_p)) %>% 
  #make vote predict as the sum of all proportion-wise predict
  group_by(agegrp) %>% 
  summarise(mean = mean(vote_predict), 
            lower = quantile(vote_predict, 0.025), 
            upper = quantile(vote_predict, 0.975))
  #return the mean of predict for each level of group(age, income, etc.)
  #the following chunk follows from the above code styles
#prediction for each income group
pre_inc <- model8 %>%
  add_predicted_draws(newdata=incgrp_p,
                      allow_new_levels=TRUE) %>%
  rename(vote_predict = .prediction) %>% 
  mutate(vote_predict_p = vote_predict*prop) %>% 
  group_by(incgrp, .draw) %>% 
  summarise(vote_predict = sum(vote_predict_p)) %>% 
  group_by(incgrp) %>% 
  summarise(mean = mean(vote_predict), 
            lower = quantile(vote_predict, 0.025), 
            upper = quantile(vote_predict, 0.975))
#prediction for each race
pre_race <- model8 %>%
  add_predicted_draws(newdata=race_p,
                      allow_new_levels=TRUE) %>%
  rename(vote_predict = .prediction) %>% 
  mutate(vote_predict_p = vote_predict*prop) %>% 
  group_by(race, .draw) %>% 
  summarise(vote_predict = sum(vote_predict_p)) %>% 
  group_by(race) %>% 
  summarise(mean = mean(vote_predict), 
            lower = quantile(vote_predict, 0.025), 
            upper = quantile(vote_predict, 0.975))
#prediction for each states
pre_state <- model8 %>%
  add_predicted_draws(newdata=state_p,
                      allow_new_levels=TRUE) %>%
  rename(vote_predict = .prediction) %>% 
  mutate(vote_predict_p = vote_predict*prop) %>% 
  group_by(statefull, .draw) %>% 
  summarise(vote_predict = sum(vote_predict_p)) %>% 
  group_by(statefull) %>% 
  summarise(mean = mean(vote_predict), 
            lower = quantile(vote_predict, 0.025), 
            upper = quantile(vote_predict, 0.975))
```

```{r agep, fig.cap = "Forcasting Votes for Trump by Age Groups", echo = FALSE, message = FALSE, fig.show="hold", out.width="80%"}
pre_age %>% 
  ggplot(aes(y = mean, x = factor(agegrp, 
                                  levels = c("Under 20",
                                             "Between 20 to 30",
                                             "Between 30 to 40",
                                             "Between 40 to 50",
                                             "Between 50 to 60",
                                             "Between 60 to 70",
                                             "Above 70")))) + 
  #factor the age groups to be in order
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + 
  scale_x_discrete(guide = guide_axis(n.dodge = 3))+
  #make x-axis variable names non-overlapping
  ylab("Proportion voting for Trump") + 
  xlab("Age in 2018")
#the estimated mean vote for trump  between different age group
```


```{r incp, fig.cap = "Forcasting Votes for Trump by Household Income Groups", echo = FALSE, fig.show="hold", message = FALSE,out.width="80%"}
pre_inc %>% 
  ggplot(aes(y = mean, x = factor(incgrp,
                                  levels = c("Less than $35,000",
                                             "$35,000 to $69,999",
                                             "$70,000 to $99,999",
                                             "$100,000 to $174,999",
                                             "$175,000 to $249,999",
                                             "More than $250,000")))) + 
  #factor income groups to be in order
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + 
  scale_x_discrete(guide = guide_axis(n.dodge = 3))+
  ylab("Proportion voting for Trump") + 
  xlab("Income level in 2018")
#the estimated mean vote for trump between different income group
```


```{r racep, fig.cap = "Forcasting Votes for Trump by Race", echo = FALSE, message = FALSE, fig.show="hold", out.width="80%"}
pre_race %>% 
  ggplot(aes(y = mean, x = forcats::fct_inorder(race))) + 
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + 
  scale_x_discrete(guide = guide_axis(n.dodge = 3))+
  ylab("Proportion voting for Trump") + 
  xlab("Race")
#the estimated mean vote for trump between different race group
```

## Forecast Electoral Vote by Support Rate in Each State

```{r statep, fig.cap = "Forcasting Votes for Trump by States", echo = FALSE, message = FALSE, fig.show="hold", out.width="80%"}
statebins(state_data = pre_state, 
          state_col = "statefull",
          value_col = "mean",
          name = "Trump Vote",
          palette = "OrRd",
          direction = 1,
          font_size=4,
          dark_label = "black", 
          light_label = "white",
          round=TRUE)+
  labs(title = "Mean Forecasted Trump Vote") + 
  theme_statebins(legend_position="right")
#create the u.s. heat map of votes for trump using statebins, higher voting likelihood for trump will be displayed more redish.
```

# Discussion

## First discussion point

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

# Appendix {-}

\newpage


# References


